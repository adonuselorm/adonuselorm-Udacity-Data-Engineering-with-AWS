# Sparkify Data Warehouse on AWS Redshift
## Introduction
Here's a professional rewrite:

# Sparkify Data Warehousing Project

## Overview
Sparkify, a rapidly growing music streaming platform, is undertaking a strategic initiative to migrate their data infrastructure to the cloud. As their user engagement and content library continue to scale exponentially, the company requires a more sophisticated approach to data management and analytics.

## Business Context
The migration to cloud infrastructure addresses several key business imperatives:
- Supporting rapid user base expansion
- Managing an ever-growing music catalog
- Enabling real-time data analytics
- Ensuring scalable and cost-effective operations

## Technical Solution
This project implements a comprehensive data warehousing solution with the following components:

1. **ETL Pipeline**: Engineered to efficiently extract data from S3, transform it according to the defined schema, and load it into Redshift
2. **Data Warehouse**: Structured as a star schema in Redshift to optimize analytical queries
3. **Cloud Infrastructure**: Leverages AWS services to ensure reliability, security, and scalability

## Business Impact
The implementation enables Sparkify to:
- Generate actionable insights from user behavior
- Optimize content recommendations
- Make data-driven business decisions
- Scale operations efficiently with growing demand

## Project Description
The primary objective of this project is to create an ETL pipeline that extracts Sparkify's data from S3, stages it in Redshift, and transforms it into a series of dimensional tables. This design will support the analytics team in gaining deeper insights into user behaviors and preferences, particularly regarding song plays.

## The ETL Process
1. **Extract:** Data is extracted from two main sources in S3:
    - **Song Data:** A collection of json files containing metadata about songs and artists. This dataset is a subset of the [Million Song Dataset](http://millionsongdataset.com/).
    - **Log Data:** User activity logs detailing interactions within the app. This dataset is being generated by an [event simulator](https://github.com/Interana/eventsim) based on the songs in the song dataset. These simulate app activity logs from an imaginary music streaming app based on configuration settings.
2. **Stage:** The extracted data is initially staged in two separate tables in Redshift:
    - **Staging Songs Table:** Holds data from the song files.
    - **Staging Events Table:** Contains log data.
3. **Transform and Load:** Data from the staging tables is transformed and loaded into a set of dimensional tables using SQL queries.

## Database Schema Design
The database uses a star schema, optimized for queries on song play analysis. This schema includes the following tables:

### Fact Table:
1. **songplays:** Records in log data associated with song plays.
    - ***songplay_id (INTEGER, PRIMARY KEY, IDENTITY):*** A unique identifier for each song play.
    - ***start_time (TIMESTAMP):*** Timestamp when the song play event occurred.
    - ***user_id (INTEGER):*** ID of the user who played the song.
    - ***level (VARCHAR):*** The subscription level (e.g., free, paid) of the user.
    - ***song_id (VARCHAR):*** ID of the song that was played.
    - ***artist_id (VARCHAR):*** ID of the artist of the song that was played.
    - ***session_id (INTEGER):*** ID of the user session during which the song was played.
    - ***location (VARCHAR):*** Location of the user at the time of the song play.
    - ***user_agent (VARCHAR):*** User agent (e.g., browser, app) used for the song play.

### Dimension Tables
1. **users:** Users in the app.
    - ***user_id (INTEGER, PRIMARY KEY):*** A unique identifier for the user.
    - ***first_name (VARCHAR):*** User's first name.
    - ***last_name (VARCHAR):*** User's last name.
    - ***gender (VARCHAR):*** User's gender.
    - ***level (VARCHAR):*** Subscription level of the user.
2. **songs:** Songs in the music database.
    - ***song_id (VARCHAR, PRIMARY KEY):*** A unique identifier for the song.
    - ***title (VARCHAR):*** Title of the song.
    - ***artist_id (VARCHAR):*** ID of the artist who performed the song.
    - ***year (INTEGER):*** Year the song was released.
    - ***duration (FLOAT):*** Duration of the song in seconds.
3. **artists:** Artists in the music database.
    - ***artist_id (VARCHAR, PRIMARY KEY):*** A unique identifier for the artist.
    - ***name (VARCHAR):*** Name of the artist.
    - ***location (VARCHAR):*** Location where the artist is based.
    - ***latitude (DECIMAL):*** Latitude of the artist's location.
    - ***longitude (DECIMAL):*** Longitude of the artist's location.
4. **time:** Timestamps of records in songplays broken down into specific units.
    - ***start_time (TIMESTAMP, PRIMARY KEY):*** Timestamp of the record.
    - ***hour (INTEGER):*** Hour extracted from start_time.
    - ***day (INTEGER):*** Day extracted from start_time.
    - ***week (INTEGER):*** Week of the year extracted from start_time.
    - ***month (INTEGER):*** Month extracted from start_time.
    - ***year (INTEGER):*** Year extracted from start_time.
    - ***weekday (VARCHAR):*** Day of the week extracted from start_time.

### Diststyle
- **Dimension Tables (users, songs, artists, time):** DISTSTYLE ALL is used because these tables are presumably smaller and frequently joined to the fact table. This style ensures that the entire content of these tables is available on every node, reducing the need for Redshift to move data across nodes for join operations.
- **Fact Table (songplays):** DISTSTYLE EVEN is chosen for balanced distribution across nodes. This is effective when there is no clear join column that dominates, or if the table is very large. It helps in parallel processing and efficient utilization of the cluster's resources.

### Conclusion
The chosen schema and column structure provide a robust foundation for analyzing Sparkify's song play data. This setup allows for efficient querying of various aspects, including user behavior, song popularity, artist reach, and listening trends over time. By leveraging this schema in AWS Redshift, Sparkify's analytics team can derive meaningful insights to drive data-driven decisions and enhance user engagement.

## End to End Execution
In order to run this project end-to-end, you have to run the following files in the respective order: 

1. Run the `IaC.ipynb` notebook cell by cell. Ensure that `aws.cfg` is filled with the correct values. 
2. Run the `create_tables.py` script. It will create all required tables in Redshift. Ensure that the file `dwh.cfg` is filled with the correct values.
3. Run the `etl.py` script.

## Example Queries and Results
### Top Three Played Songs
It is always interesting to see what songs the users like most. The following query gets the top three songs of the songplay database.

```sql
SELECT s.title, a.name AS artist_name, COUNT(*) as play_count
FROM songplays sp
JOIN songs s ON sp.song_id = s.song_id
JOIN artists a ON sp.artist_id = a.artist_id
GROUP BY s.title, a.name
ORDER BY play_count DESC
LIMIT 3;
```

**Result:**
1. You're The One from Dwight Yoakam with a play count of **37**.
2. I CAN'T GET STARTED from Ron Carter with a play count of **9**.
3. Catch You Baby (Steve Pitron & Max Sanna Radio Edit) from Lonnie Gordon with a play count of **9**.

### Peak Usage Time of Day
It is also interesting to see what time of the day the users are most active in. This helps in case new songs are released to directly draw more attention if released during peak times. 

```sql
SELECT t.hour, COUNT(*) as play_count
FROM songplays sp
JOIN time t ON sp.start_time = t.start_time
GROUP BY t.hour
ORDER BY play_count DESC
LIMIT 2;
```

**Result**
The peak hour is hour **17**, which means 5 pm. This also makes sense, as this is the time where most people start to get off work and are on their way home and can listen to music. The second most active hour is hour **18**. 
